{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c2e017-b833-4ee2-ab49-01fc4a57fa76",
   "metadata": {},
   "source": [
    "# Reflexion Orchestration Agent\n",
    "\n",
    "## Overview\n",
    "In this example we will guide you through how to create a Reflexion pattern implementation using Strands multiagent orchestration. We will demonstrate an iterative self-improvement system that generates initial responses, critically evaluates them, and refines them through systematic reflection until reaching acceptable quality standards.\n",
    "\n",
    "## Agent Details\n",
    "<div style=\"float: left; margin-right: 20px;\">\n",
    "    \n",
    "|Feature             |Description                                        |\n",
    "|--------------------|---------------------------------------------------|\n",
    "|Native tools used   |generate_initial_answer, generate_revised_answer  |\n",
    "|Custom tools created|None (uses built-in reflection capabilities)      |\n",
    "|Agent Structure     |Multi-agent orchestration with feedback loops     |\n",
    "|AWS services used   |Amazon Bedrock                                     |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775e637-0a0d-4193-a44f-facf41e14398",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./images/reflexion.png\" alt=\"Reflexion Architecture\" width=\"800\">\n",
    "    <p>The system consists of two specialized agents connected in an iterative feedback loop:</p>\n",
    "    <p><em>Reflexion Architecture: User Query → [Draft Agent] ⟷ [Revisor Agent] → Final Response</em></p>\n",
    "</div>\n",
    "\n",
    "## Key Features\n",
    "\n",
    "* **Iterative self-improvement**: Implements reflection and revision cycles for quality enhancement through systematic feedback loops\n",
    "* **Multi-agent orchestration**: Creates a system with Draft Agent and Revisor Agent working in iterative sequence\n",
    "* **Quality assessment**: Multi-dimensional evaluation including completeness, clarity, and actionability with configurable thresholds\n",
    "* **State management**: Maintains revision state across iterations for consistent improvement and progress tracking\n",
    "* **Feedback loops**: Internal revision loops with configurable maximum iterations to prevent infinite cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68566b50-ccba-4c9d-822e-7062fc6c07d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install -r ./requirements.txt --quiet --upgrade\n",
    "!pip3 install strands-agents strands-agents-tools --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7802c-3565-4520-9ad9-33fbc0f1f5b4",
   "metadata": {},
   "source": [
    "## Importing dependency packages\n",
    "\n",
    "Now let's import all the necessary libraries and modules for our Reflexion implementation. This includes standard Python libraries, AWS SDK components, Strands framework modules, and custom helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6caacd-3fc8-4063-8763-a64e89a47b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import ipywidgets as widgets\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sqlite3\n",
    "import functools\n",
    "import requests\n",
    "import pytz\n",
    "import warnings\n",
    "from IPython.display import Image, display\n",
    "from botocore.config import Config\n",
    "from typing import Annotated, Literal, Optional, Union\n",
    "from typing_extensions import TypedDict\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, datetime\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "from strands import Agent\n",
    "from strands import tool\n",
    "from strands.models import BedrockModel\n",
    "from strands.agent.conversation_manager import SlidingWindowConversationManager\n",
    "\n",
    "from strands.multiagent.graph import GraphBuilder\n",
    "from strands.agent import AgentResult\n",
    "from strands.types.content import Message\n",
    "from strands.types.streaming import StopReason\n",
    "from strands.telemetry.metrics import EventLoopMetrics\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa4f23-579b-4c8d-9758-3f4d9bdfeadd",
   "metadata": {},
   "source": [
    "## Import airline domain tools\n",
    "\n",
    "Now we'll import the comprehensive set of airline domain tools from MAbench and TauBench. These tools provide the actual functionality that our Reflexion agent will execute, including flight booking, reservation management, and customer service operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51405b31-e274-4a9a-9266-a9cb85077099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../data/ma-bench/')\n",
    "sys.path.append('../data/tau-bench/')\n",
    "\n",
    "from mabench.environments.airline.tools.book_reservation import book_reservation\n",
    "from mabench.environments.airline.tools.calculate import calculate\n",
    "from mabench.environments.airline.tools.cancel_reservation import cancel_reservation\n",
    "from mabench.environments.airline.tools.get_reservation_details import get_reservation_details\n",
    "from mabench.environments.airline.tools.get_user_details import get_user_details\n",
    "from mabench.environments.airline.tools.list_all_airports import list_all_airports\n",
    "from mabench.environments.airline.tools.search_direct_flight import search_direct_flight\n",
    "from mabench.environments.airline.tools.search_onestop_flight import search_onestop_flight\n",
    "from mabench.environments.airline.tools.send_certificate import send_certificate\n",
    "from mabench.environments.airline.tools.think import think\n",
    "from mabench.environments.airline.tools.transfer_to_human_agents import transfer_to_human_agents\n",
    "from mabench.environments.airline.tools.update_reservation_baggages import update_reservation_baggages\n",
    "from mabench.environments.airline.tools.update_reservation_flights import update_reservation_flights\n",
    "from mabench.environments.airline.tools.update_reservation_passengers import update_reservation_passengers\n",
    "\n",
    "domain = \"airline\"\n",
    "\n",
    "from tau_bench.envs.airline.data import *\n",
    "from tau_bench.envs.airline.tasks import *\n",
    "from tau_bench.envs.airline.wiki import WIKI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf713768-eb39-4046-94fe-de9a56538115",
   "metadata": {},
   "source": [
    "## Configure Strands Framework\n",
    "\n",
    "Now let's set up the core Strands framework components that will power our Reflexion multiagent system. We need to configure the AWS Bedrock connection, conversation management, and logging to ensure our three-agent pipeline runs smoothly.\n",
    "\n",
    "### Framework Setup Process\n",
    "\n",
    "First, we'll establish the **AWS region** and create a `BedrockModel` instance that the two  Reflexion agents (Draft and Revisor) will share. We do this so that all agents use the same LLM configuration for consistent behavior. You canalso use different LLM configurations.\n",
    "\n",
    "Finally, we'll configure **logging** to minimize noise during execution so we can focus on the Reflexion execution flow and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea86c4-fe47-464b-8841-0ec3d2d6bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "\n",
    "bedrock_model_taubench = BedrockModel(region_name=region)\n",
    "\n",
    "# Disable logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "for logger_name in [\"strands\", \"graph\", \"event_loop\", \"registry\", \"sliding_window_conversation_manager\", \"bedrock\", \"streaming\"]:\n",
    "    logging.getLogger(logger_name).setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72152c-bc50-4d40-952e-98db54c1cea3",
   "metadata": {},
   "source": [
    "## Reflexion with State Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcbdcd1-0c4d-4e72-9734-aaad86fdc7d5",
   "metadata": {},
   "source": [
    "## Define the Reflexion State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReflexionState:\n",
    "    \"\"\"State for reflexion workflow\"\"\"\n",
    "    user_query: str = \"\"\n",
    "    response: str = \"\"\n",
    "    reflection: str = \"\"\n",
    "    needs_revision: bool = False\n",
    "    max_iterations: int = 5\n",
    "    revision_count: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207d8b8-c4d2-42e0-a9ba-b24c72e4e1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_prompt(prompt) -> str:\n",
    "    if isinstance(prompt, list):\n",
    "        # assume list of dicts like [{\"text\": \"...\"}]\n",
    "        texts = [p.get(\"text\", \"\") for p in prompt if isinstance(p, dict)]\n",
    "        return \" \".join(t.strip() for t in texts if t)\n",
    "    if isinstance(prompt, dict) and \"text\" in prompt:\n",
    "        return prompt[\"text\"].strip()\n",
    "    if isinstance(prompt, str):\n",
    "        return prompt.strip()\n",
    "    return str(prompt).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4af57-5102-4e9e-956f-0b64f5c975b0",
   "metadata": {},
   "source": [
    "## Create Common Agent and Prompts \n",
    "We will first create the flight tool executor agent and the reflection prompt which will be used by the 2 different nodes of the Reflexion graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4114a6-74e5-45ff-8d9a-407c605478a8",
   "metadata": {},
   "source": [
    "### Create Flight Tool Executor Agent\n",
    "\n",
    "Now let's create a comprehensive flight tool executor that serves as our baseline ReAct agent. This agent has direct access to all airline tools and uses intelligent reasoning to handle customer queries without the structured planning approach of REWOO.\n",
    "\n",
    "\n",
    "#### System Prompt Configuration\n",
    "\n",
    "First, we define a **system prompt template** that establishes the agent's role and operational guidelines.The prompt includes **policy integration** from the airline WIKI to ensure compliance with business rules, **geographic validation** to verify airport locations match user-specified states, and **accuracy requirements** to prevent hallucinated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea1a38-334a-40e8-879a-a354226a170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = \"\"\"\n",
    "You are a helpful assistant for a travel website. Help the user answer any questions.\n",
    "\n",
    "<instructions>\n",
    "-You MUST refer the <policy> to follow the guidelines to answer user question accurately\n",
    "- Remeber to check if the the airport city is in the state mentioned by the user. For example, Houston is in Texas.\n",
    "- Infer about the the U.S. state in which the airport city resides. For example, Houston is in Texas.\n",
    "- You should not use made-up or placeholder arguments.\n",
    "<instructions>\n",
    "\n",
    "<policy>\n",
    "{policy}\n",
    "</policy>\n",
    "\"\"\"\n",
    "\n",
    "prompt = system_prompt_template.replace(\"{policy}\", WIKI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0447486-0e60-4d5c-bbb4-3c47becdeac3",
   "metadata": {},
   "source": [
    "#### FlightToolExecutor Class\n",
    "\n",
    "Next, we create a custom agent class that extends the base `Agent` with comprehensive airline tool access. This agent has access to the **complete airline tool suite** including booking, search, update, and customer service operations. It uses the **shared Bedrock model** for consistency and incorporates **policy-aware prompting** to ensure compliant responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d5f1e-1449-455e-b2f5-dc88146609e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FlightToolExecutor(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model=bedrock_model_taubench,\n",
    "            system_prompt=prompt,\n",
    "            tools=[\n",
    "                book_reservation, calculate, cancel_reservation, get_reservation_details,\n",
    "                get_user_details, list_all_airports, search_direct_flight, search_onestop_flight,\n",
    "                send_certificate, think, transfer_to_human_agents, update_reservation_baggages,\n",
    "                update_reservation_flights, update_reservation_passengers\n",
    "            ]\n",
    "        )\n",
    "\n",
    "flight_executor = FlightToolExecutor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f8f65-ce2f-4bd6-9449-891046930fa7",
   "metadata": {},
   "source": [
    "#### Define Reflection System Prompt\n",
    "\n",
    "Now let's establish the specialized system prompt that guides our reflection agent in critically evaluating flight assistant responses. This prompt ensures comprehensive quality assessment across multiple dimensions to determine if responses need improvement.\n",
    "\n",
    "#### Multi-Dimensional Quality Assessment\n",
    "\n",
    "The `reflection_system_prompt` defines a structured evaluation framework that analyzes flight assistant responses on key quality dimensions:\n",
    "\n",
    "- **Completeness**\n",
    "- **Final Answer**\n",
    "- **Clarity**\n",
    "- **Actionability**\n",
    "- **User Experience**\n",
    "- **Missing Information**\n",
    "\n",
    "#### Decision Framework\n",
    "\n",
    "The prompt concludes with a **binary decision mechanism** (REVISE or ACCEPT) along with reasoning, enabling the Reflexion system to make informed decisions about whether responses meet quality standards or require iterative improvement through additional revision cycles.\n",
    "\n",
    "This systematic approach ensures that our Reflexion agent maintains consistent evaluation criteria while focusing on real database-driven responses rather than hallucinated information. You can change this prompt to suit your usecase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2d9d6-da18-4cc2-9445-81e1ddc2fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_system_prompt=\"\"\"You are analyzing a flight assistant's response that uses real flight database tools.\n",
    "        \n",
    "IMPORTANT: The flight data comes from real database queries, NOT hallucination.\n",
    "        \n",
    "Analyze the response quality on these dimensions:\n",
    "1. **Completeness**: Does it address all parts of the user's query? \n",
    "2. **Final Answer**: If the user query clearly states the final goal and if it can be fulfiled as per the policy, then does the response show that?\n",
    "2. **Clarity**: Is the information presented clearly and logically?\n",
    "3. **Actionability**: Are next steps or options clearly presented?\n",
    "4. **User Experience**: Is the tone helpful and appropriate?\n",
    "5. **Missing Information**: What important details are missing?\n",
    "6. **Decision**: REVISE or ACCEPT\n",
    "7. **Reason**: Why this decision was made\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e20878-ff7d-47a0-b723-665f45114d4f",
   "metadata": {},
   "source": [
    "## Draft Node\n",
    "\n",
    "Now let's create the draft node. This node has access to the `generate_initial_answer` tool and serves as the entry point for our Reflexion system, generating initial responses with built-in self-reflection capabilities.\n",
    "\n",
    "### Create Initial Answer Generation Tool\n",
    "\n",
    "Now let's implement the core Reflexion tool that generates initial responses and performs self-reflection. This tool combines the baseline flight executor with a specialized reflection agent to evaluate response quality and determine if revision is needed.\n",
    "\n",
    "### Initial Response with Self-Reflection\n",
    "\n",
    "The `generate_initial_answer` tool implements the first stage of the Reflexion pattern by following a structured process:\n",
    "\n",
    "1. **Initial Response Generation**: Uses the baseline `flight_executor` to generate an initial answer with full airline tool access\n",
    "2. **Critical Self-Reflection**: Creates a specialized reflection agent that evaluates the response quality using the `reflection_system_prompt`\n",
    "3. **Revision Decision**: Analyzes the reflection text to determine if the response needs improvement (looks for \"REVISE\" keyword)\n",
    "4. **Structured Output**: Returns a formatted response containing the original answer, reflection analysis, and revision decision\n",
    "\n",
    "This tool forms the foundation of our Reflexion system, enabling **systematic self-evaluation** of response quality before presenting results to users. The reflection agent critically examines aspects like completeness, accuracy, and helpfulness to determine if iterative improvement is necessary.\n",
    "\n",
    "### Draft Agent Implementation\n",
    "\n",
    "The Draft Agent extends the base `Agent` class with custom `stream_async()` methods required for multiagent graph compatibility. It uses the shared Bedrock model and has access to the `generate_initial_answer` tool, making it the starting point for all Reflexion workflows where initial responses are generated and evaluated for quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f81739-1fc4-4883-b363-1dc51a5064d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def generate_initial_answer(query: str) -> str:\n",
    "    \"\"\"Generate initial answer, reflect, and decide if revision needed\"\"\"\n",
    "    \n",
    "    flight_response = flight_executor(query)\n",
    "    answer_text = str(flight_response)\n",
    "    \n",
    "    reflection_agent = Agent(\n",
    "        model=bedrock_model_taubench,\n",
    "        system_prompt=reflection_system_prompt\n",
    "    )\n",
    "    \n",
    "    reflection_prompt = f\"\"\"\n",
    "Original Query: {query}\n",
    "Flight Assistant's Answer: {answer_text}\n",
    "\n",
    "Remember: The flight data comes from real database queries.\n",
    "Please provide a critical reflection of this answer:\"\"\"\n",
    "    \n",
    "    reflection_response = reflection_agent(reflection_prompt)\n",
    "    reflection_text = str(reflection_response)\n",
    "    \n",
    "    needs_revision = \"REVISE\" in reflection_text.upper()\n",
    "    \n",
    "    return f\"**Answer**: {answer_text}\\n**Self-Reflection**: {reflection_text}\\n**Needs Revision**: {needs_revision}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071e49a-ae5e-4ee8-9903-e9f96aa79ec2",
   "metadata": {},
   "source": [
    "## Revisor Agent\n",
    "\n",
    "Now let's explore the **Revisor Agent**, which serves as the iterative improvement engine in our Reflexion pattern. This agent takes the initial response from the Draft Agent and continuously refines it through self-reflection and query optimization until the quality meets our standards.\n",
    "\n",
    "### Query Improvement System\n",
    "\n",
    "The Revisor Agent starts with a specialized query improvement system that analyzes reflection feedback and creates better queries to guide improved responses. We do this so that the agent can address specific issues identified during self-reflection rather than just regenerating the same type of response.\n",
    "\n",
    "The `query_improver_system_prompt` teaches the agent to transform vague or problematic queries into more specific, actionable ones. For example, if the original query was \"Book me a flight from NYC to LA tomorrow\" and the reflection identified that \"Agent booked immediately without showing options\", the improved query becomes \"Please SEARCH and SHOW ME available flight options from NYC to LA tomorrow. I want to see different times, prices, and airlines before deciding. DO NOT book anything until I confirm.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba5a6e-a169-4ecd-abab-f2ad299723af",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_improver_system_prompt=\"\"\"You are a query improvement specialist. Based on reflection analysis, improve the original user query to address identified issues and guide better responses.\n",
    "\n",
    "Examples:\n",
    "Original: \"Book me a flight from NYC to LA tomorrow\"\n",
    "Issue: \"Agent booked immediately without showing options\"\n",
    "Improved: \"Please SEARCH and SHOW ME available flight options from NYC to LA tomorrow. I want to see different times, prices, and airlines before deciding. DO NOT book anything until I confirm.\"\n",
    "\n",
    "Now improve the provided query based on the specific reflection issues identified.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444568fe-79f3-46fd-be06-55f855c6cb69",
   "metadata": {},
   "source": [
    "\n",
    "### Response Analysis and Extraction\n",
    "\n",
    "The `extract_answer_reflection_revision` function parses the structured output from the revision process to extract four key components. We do this so that we can programmatically determine whether another revision cycle is needed and what specific improvements should be made.\n",
    "\n",
    "This function uses regex patterns to extract:\n",
    "- **Answer**: The actual response content\n",
    "- **Self-Reflection**: The agent's analysis of its own response quality\n",
    "- **Needs Revision**: Boolean flag indicating if further improvement is required\n",
    "- **User Query**: The current query being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488c8cb-5e80-4545-be96-60fc415034d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_answer_reflection_revision(tool_result):\n",
    "    content_text = tool_result[\"content\"][0][\"text\"]\n",
    "\n",
    "    answer_match = re.search(r\"\\*\\*Answer\\*\\*:(.*?)(?=\\*\\*Self-Reflection\\*\\*:)\", content_text, re.DOTALL)\n",
    "    answer = answer_match.group(1).strip() if answer_match else \"[Not found]\"\n",
    "\n",
    "    reflection_match = re.search(r\"\\*\\*Self-Reflection\\*\\*:(.*?)(?=\\*\\*Needs Revision\\*\\*:|$)\", content_text, re.DOTALL)\n",
    "    self_reflection = reflection_match.group(1).strip() if reflection_match else \"[Not found]\"\n",
    "\n",
    "    revision_match = re.search(r\"\\*\\*Needs Revision\\*\\*:\\s*(True|False)\", content_text)\n",
    "    needs_revision = revision_match.group(1) == \"True\" if revision_match else False\n",
    "\n",
    "    query_match = re.search(r\"\\*\\*User-Query\\*\\*:\\s*(.*?)(?=\\n|$)\", content_text)\n",
    "    query = query_match.group(1).strip() if query_match else \"[Not found]\"\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"self_reflection\": self_reflection,\n",
    "        \"needs_revision\": needs_revision,\n",
    "        \"user_query\": query\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1d9a5-aa22-4195-9222-e7329af849c5",
   "metadata": {},
   "source": [
    "### Revised Answer Generation Tool\n",
    "\n",
    "The `generate_revised_answer` tool orchestrates the complete revision cycle by combining query improvement, response regeneration, and quality assessment. We implement this as a single tool so that all revision steps happen atomically and maintain consistency throughout the improvement process.\n",
    "\n",
    "The tool follows this workflow:\n",
    "1. **Query Improvement**: Uses the query improver agent to create a better version of the user query based on current reflection feedback\n",
    "2. **Response Regeneration**: Executes the improved query using the flight executor to generate a revised answer\n",
    "3. **Quality Assessment**: Applies the reflection system prompt to analyze the new response and determine if further revision is needed\n",
    "4. **Structured Output**: Returns all components in the standardized format for the next iteration\n",
    "\n",
    "### Iterative Improvement Process\n",
    "\n",
    "The Revisor Agent creates a feedback loop where each iteration builds upon the insights from previous attempts. We do this so that the system can progressively address different quality issues rather than getting stuck on the same problems.\n",
    "\n",
    "The revision decision is made by checking if \"REVISE\" appears in the reflection text, providing a simple but effective mechanism for the agent to signal when it believes the response quality is sufficient. This approach allows the system to naturally converge on high-quality responses while preventing infinite revision loops.\n",
    "\n",
    "The Revisor Agent represents the core innovation of the Reflexion pattern - the ability to iteratively improve responses through structured self-reflection and targeted query refinement, ensuring that the final output meets quality standards before being presented to the user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acfe2e-0706-49bf-977c-ab129c0d7e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def generate_revised_answer(current_user_query, current_response=\"\", current_reflection=\"\") -> str:\n",
    "    \"\"\"Generate revised answer, reflect, and decide if further revision needed\"\"\"\n",
    "    \n",
    "    query_improver = Agent(\n",
    "        model=bedrock_model_taubench,\n",
    "        system_prompt=query_improver_system_prompt\n",
    "    )\n",
    "    \n",
    "    improved_query = query_improver(f\"Current user query: {current_user_query}\\nCurrent Response: {current_response}\\nReflection: {current_reflection}\\nCreate better query:\")\n",
    "    \n",
    "    flight_response = flight_executor(str(improved_query))\n",
    "    revised_answer = str(flight_response)\n",
    "    \n",
    "    revision_reflection_agent = Agent(\n",
    "        model=bedrock_model_taubench,\n",
    "        system_prompt=reflection_system_prompt\n",
    "    )\n",
    "    \n",
    "    new_reflection = revision_reflection_agent(f\"Task: {current_user_query} Revised Answer: {revised_answer} Analyze and decide:\")\n",
    "    reflection_text = str(new_reflection)\n",
    "    \n",
    "    needs_revision = \"REVISE\" in reflection_text.upper()\n",
    "    \n",
    "    return f\"\\n**User-Query**: {current_user_query}\\n**Answer**: {revised_answer}\\n**Self-Reflection**: {reflection_text}\\n**Needs Revision**: {needs_revision}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88552597-f09a-4f9b-be5b-0764cedf3205",
   "metadata": {},
   "source": [
    "## Custom Agent Classes\n",
    "\n",
    "Now let's examine the **custom agent classes** that implement the Reflexion pattern within the Strands multiagent graph framework. These agents extend the base Agent class with specialized streaming behavior to handle the iterative reflection and revision process.\n",
    "\n",
    "### Draft Agent Implementation\n",
    "\n",
    "The `DraftAgent` class serves as the entry point for our Reflexion system, generating initial responses with built-in self-reflection capabilities. We implement this as a custom agent class so that it can seamlessly integrate with the multiagent graph while providing the structured output format required for the revision process.\n",
    "\n",
    "The agent initializes with the `generate_initial_answer` tool and uses the `bedrock_model_taubench` for consistent model behavior across the system. We do this so that both agents in the Reflexion pattern use the same underlying language model for coherent reasoning.\n",
    "\n",
    "In the `stream_async` method, the agent normalizes the input prompt and calls its tool to generate the initial response with self-reflection. The `extract_answer_reflection_revision` function then parses the structured output to separate the answer, reflection, and revision decision components. We structure it this way so that the multiagent graph can easily pass the parsed components to the next agent in the workflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c56241-d51f-4472-9123-c9f6b6ef6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DraftAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model=bedrock_model_taubench,\n",
    "            tools=[generate_initial_answer],\n",
    "            name=\"draft\",\n",
    "            description=\"Generates flight assistance answers with self-reflection\"\n",
    "        )\n",
    "    \n",
    "    async def stream_async(self, prompt: str):\n",
    "\n",
    "        prompt=normalize_prompt(prompt)        \n",
    "        result = self.tool.generate_initial_answer(query=prompt)        \n",
    "        extracted = extract_answer_reflection_revision(result)\n",
    "        \n",
    "        message = Message(content=[{\"text\": str(result)}])\n",
    "        print(\"DEBUG: REVISOR AGENT RESULT: \\n\", json.dumps(message), \"\\n\")\n",
    "        agent_result = AgentResult(\n",
    "            stop_reason=\"end_turn\",\n",
    "            message=message,\n",
    "            metrics=EventLoopMetrics(),\n",
    "            state=None \n",
    "        )\n",
    "        yield {\"result\": agent_result}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faac25c-621c-458d-9432-88a44f1937c8",
   "metadata": {},
   "source": [
    "### Revisor Agent Implementation\n",
    "\n",
    "The `RevisorAgent` class handles the iterative improvement process, managing state across multiple revision cycles until the response quality is satisfactory. We implement this with sophisticated state management so that the agent can track revision history and prevent infinite loops.\n",
    "\n",
    "The agent maintains a `ReflexionState` object that tracks the user query, current response, reflection analysis, revision status, and iteration count. We do this so that each revision cycle builds upon previous insights rather than starting from scratch.\n",
    "\n",
    "### State Management and Flow Control\n",
    "\n",
    "The revisor agent's `stream_async` method implements complex input parsing to handle both initial queries and continuation from the draft agent. When receiving input from the draft agent (identified by the \"From draft:\" marker), it extracts the previous response and reflection to initialize the state properly.\n",
    "\n",
    "The revision loop continues as long as `needs_revision` is true and the `revision_count` hasn't exceeded `max_iterations` (set to 5). We implement this safeguard so that the system gracefully handles cases where the agent cannot achieve satisfactory quality within reasonable bounds.\n",
    "\n",
    "### Multiagent Graph Integration\n",
    "\n",
    "Both agent classes yield `AgentResult` objects with properly formatted messages and state information. The draft agent passes its results forward, while the revisor agent can either continue revising or provide the final polished response. We structure the results this way so that the multiagent graph can properly route information between agents and maintain conversation flow.\n",
    "\n",
    "The debug print statements help track the flow of information between agents during development and troubleshooting. These custom agent implementations demonstrate how the Strands framework can be extended to support sophisticated multi-turn reasoning patterns like Reflexion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43ed3f-cd8d-4d33-9d49-3bbad5fb4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevisorAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            model=bedrock_model_taubench,\n",
    "            tools=[generate_revised_answer],\n",
    "            name=\"revisor\",\n",
    "            description=\"Revises flight responses\"\n",
    "        )\n",
    "\n",
    "    async def stream_async(self, input_data):\n",
    "        input_data=normalize_prompt(input_data)  \n",
    "        if isinstance(input_data, str):\n",
    "            state = ReflexionState(user_query=input_data)\n",
    "        else:\n",
    "            prev_state = getattr(input_data, 'state', {}) or {}\n",
    "            print(f\"PREV STATE FROM REVIOSR: {prev_state} \\n\")\n",
    "            state = ReflexionState(**prev_state) if prev_state else ReflexionState()\n",
    "\n",
    "        # Extract draft agent result\n",
    "        draft_start = input_data.find('From draft:')\n",
    "        if draft_start != -1:\n",
    "            draft_content = input_data[draft_start + len('From draft:'):].strip()\n",
    "            # Parse the draft result\n",
    "            extracted = extract_answer_reflection_revision({'content': [{'text': draft_content}]})\n",
    "            draft_response = extracted['answer']\n",
    "            draft_reflection = extracted['self_reflection']\n",
    "            needs_revision = extracted['needs_revision']\n",
    "\n",
    "        state = ReflexionState(\n",
    "            user_query=user_query,\n",
    "            response=draft_response,\n",
    "            reflection=draft_reflection,\n",
    "            needs_revision=needs_revision,\n",
    "            revision_count=0,\n",
    "            max_iterations=5\n",
    "        )\n",
    "        \n",
    "        print(f\"Revisor starting: revision_count={state.revision_count}, needs_revision={state.needs_revision}\")\n",
    "        \n",
    "        if state.needs_revision and state.revision_count < state.max_iterations:\n",
    "            state.revision_count += 1\n",
    "            \n",
    "            result = self.tool.generate_revised_answer(\n",
    "                current_user_query=state.user_query,\n",
    "                current_response=state.response,\n",
    "                current_reflection=state.reflection\n",
    "            )\n",
    "            \n",
    "            extracted = extract_answer_reflection_revision(result)\n",
    "            state.response = extracted[\"answer\"]\n",
    "            state.reflection = extracted[\"self_reflection\"]\n",
    "            state.needs_revision = extracted[\"needs_revision\"]\n",
    "        else:\n",
    "            result = f\"**Answer**: {state.response}\\n**Final Reflection**: {state.reflection}\\n**Revision Complete**: After {state.revision_count} revisions\"\n",
    "        \n",
    "        message = Message(content=[{\"text\": str(result)}])\n",
    "        print(\"DEBUG: REVISOR AGENT RESULT: \\n\", json.dumps(message), \"\\n\")\n",
    "        agent_result = AgentResult(\n",
    "            stop_reason=\"end_turn\",\n",
    "            message=message,\n",
    "            metrics=EventLoopMetrics(),\n",
    "            state=state.__dict__\n",
    "        )\n",
    "        yield {\"result\": agent_result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8463c-09f9-4433-92ba-1663a83798c5",
   "metadata": {},
   "source": [
    "## Build Reflexion Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483e46b-c8ab-4268-95fa-f2761472d351",
   "metadata": {},
   "source": [
    "## Graph Construction and Execution\n",
    "\n",
    "Now let's create the **Reflexion multiagent graph** that orchestrates the interaction between our Draft and Revisor agents. This function builds the complete workflow using the Strands GraphBuilder to create a seamless reflection and revision pipeline.\n",
    "\n",
    "### Graph Creation Function\n",
    "\n",
    "The `create_reflexion_graph` function instantiates both agent classes and connects them in a sequential workflow. We do this so that the draft agent's output automatically flows to the revisor agent for quality assessment and potential improvement.\n",
    "\n",
    "First, we create instances of both `DraftAgent` and `RevisorAgent` classes, ensuring they're properly initialized with their respective tools and configurations. We instantiate them separately so that each agent maintains its own state and tool access throughout the workflow.\n",
    "\n",
    "### Graph Builder Configuration\n",
    "\n",
    "Using the Strands `GraphBuilder`, we add both agents as nodes in our multiagent graph. The `add_node` method registers each agent with a unique identifier (\"draft\" and \"revisor\") that allows the graph to route messages and maintain execution flow.\n",
    "\n",
    "We then establish the connection between agents using `add_edge(draft_node, revisor_node)`, creating a direct path from the draft agent's output to the revisor agent's input. We do this so that the reflection process happens automatically without requiring manual intervention or complex routing logic.\n",
    "\n",
    "### Entry Point and Execution Flow\n",
    "\n",
    "The `set_entry_point(\"draft\")` call designates the draft agent as the starting point for all user queries. We configure it this way so that every interaction begins with initial response generation, followed by the reflection and revision process.\n",
    "\n",
    "When the graph executes, it follows this flow:\n",
    "1. User query enters at the draft agent\n",
    "2. Draft agent generates initial response with self-reflection\n",
    "3. Output automatically routes to revisor agent\n",
    "4. Revisor agent performs iterative improvement until quality standards are met\n",
    "5. Final polished response is returned to the user\n",
    "\n",
    "### Graph Instantiation\n",
    "\n",
    "The final line `reflexion_graph = create_reflexion_graph()` builds and stores the complete multiagent graph, making it ready for execution. We create this as a reusable object so that multiple queries can be processed through the same reflection pipeline without rebuilding the graph structure.\n",
    "\n",
    "This simple but powerful setup demonstrates how the Strands multiagent graph framework can orchestrate complex reasoning patterns like Reflexion with minimal configuration code, while maintaining full control over agent behavior and state management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40521cd2-b8c6-4aa3-a0b6-742830d054cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.multiagent.graph import GraphBuilder, GraphState\n",
    "\n",
    "def create_reflexion_graph():\n",
    "    \"\"\"Create reflexion graph with state management\"\"\"\n",
    "    \n",
    "    draft_agent = DraftAgent()\n",
    "    revisor_agent = RevisorAgent()\n",
    "    \n",
    "    builder = GraphBuilder()\n",
    "    \n",
    "    draft_node = builder.add_node(draft_agent, \"draft\")\n",
    "    revisor_node = builder.add_node(revisor_agent, \"revisor\")\n",
    "    \n",
    "    builder.add_edge(draft_node, revisor_node)\n",
    "    builder.set_entry_point(\"draft\")\n",
    "    \n",
    "    return builder.build()\n",
    "\n",
    "reflexion_graph = create_reflexion_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bddef-19b6-4f20-ae14-2710dd1b432b",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Now let's load the **TauBench evaluation dataset** that contains real airline customer service scenarios. We do this so that we can test our Reflexion system against standardized benchmarks and measure its performance on authentic customer queries like:\n",
    "\n",
    "- Flight changes\n",
    "- Cancellations  \n",
    "- Booking modifications\n",
    "\n",
    "This loads the **single-turn airline tasks** from TauBench, which provides us with a collection of customer queries along with their expected outcomes for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02191e41-e024-4355-8ecd-6c24606fab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(\"..\", \"data\", \"tau-bench\", \"tau_bench\", \"envs\", f\"{domain}\", \"tasks_singleturn.json\")\n",
    "with open(output_path, \"r\") as file:\n",
    "    tasks = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c84fb-81e1-4bd7-9470-fcdf99775b04",
   "metadata": {},
   "source": [
    "## Testing and Evaluation\n",
    "\n",
    "Now let's examine the **testing framework** that demonstrates how to execute and analyze the Reflexion graph performance. This function provides comprehensive insights into the graph execution process and helps validate the quality improvement achieved through the reflection pattern.\n",
    "\n",
    "### Test Function Setup\n",
    "\n",
    "The `test_reflexion_graph` function creates a fresh instance of the reflexion graph for each test execution. We do this so that each test starts with a clean state and doesn't carry over any residual information from previous executions.\n",
    "\n",
    "The function begins by printing the test prompt and execution status, providing clear visibility into what query is being processed. We include this logging so that developers can track the progression of different test cases and identify any patterns in the reflection behavior.\n",
    "\n",
    "### Performance Monitoring\n",
    "\n",
    "The test function captures execution timing using `time.time()` measurements around the graph execution. We measure this so that we can evaluate the performance impact of the reflection process compared to single-pass approaches.\n",
    "\n",
    "The timing measurement helps identify whether the quality improvements from reflection justify the additional computational cost, which is crucial for production deployment decisions.\n",
    "\n",
    "### Graph Execution Analysis\n",
    "\n",
    "After executing the graph with the user query, the function extracts comprehensive execution metadata from the result object. We analyze these metrics so that we can understand how the multiagent graph performed and whether both agents completed successfully.\n",
    "\n",
    "The key metrics include:\n",
    "- **Graph Status**: Overall execution success or failure\n",
    "- **Total Nodes**: Number of agents in the graph (should be 2 for our Reflexion pattern)\n",
    "- **Completed Nodes**: How many agents finished execution\n",
    "- **Execution Order**: The sequence in which agents were invoked\n",
    "\n",
    "### Node-Level Result Inspection\n",
    "\n",
    "The function iterates through each node's results to display the individual agent outputs. We examine each node separately so that we can trace the evolution from initial draft response to final refined answer.\n",
    "\n",
    "For each agent (draft and revisor), the function shows the execution status and result content, allowing developers to see exactly how the reflection process improved the response quality.\n",
    "\n",
    "### Test Execution Example\n",
    "\n",
    "The final section demonstrates how to run a test using a specific question from the tasks dataset. We select `question_id = 43` and extract the corresponding user query to test the system with real airline customer service scenarios.\n",
    "\n",
    "This testing approach provides a complete view of the Reflexion pattern in action, showing both the technical execution details and the practical quality improvements achieved through iterative self-reflection and revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10459f3-1830-4f91-afd9-ad03e69de189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reflexion_graph(user_query):\n",
    "    reflexion_graph = create_reflexion_graph()\n",
    "    \n",
    "    print(\"=== Testing Reflexion Graph ===\")\n",
    "    print(f\"Test Prompt: {user_query}\")\n",
    "    print(\"\\n--- Executing Graph ---\")\n",
    "    start= time.time()\n",
    "    result = reflexion_graph(user_query)\n",
    "    exec_time= time.time()-start\n",
    "    print(f\"\\n EXEC Time: {exec_time}\")\n",
    "    print(f\"\\nGraph Status: {result.status}\")\n",
    "    print(f\"Total Nodes: {result.total_nodes}\")\n",
    "    print(f\"Completed Nodes: {result.completed_nodes}\")\n",
    "    print(f\"Execution Order: {[node.node_id for node in result.execution_order]}\")\n",
    "    \n",
    "    print(\"\\n--- Node Results ---\")\n",
    "    for node_id, node_result in result.results.items():\n",
    "        print(f\"\\n{node_id.upper()}:\")\n",
    "        print(f\"Status: {node_result.status}\")\n",
    "        if hasattr(node_result, 'result') and node_result.result:\n",
    "            print(f\"Result: {str(node_result.result)}...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with different question_id\n",
    "question_id = 43\n",
    "task = tasks[question_id]\n",
    "user_query = task[\"question\"]\n",
    "print(user_query)\n",
    "reflexion_response = test_reflexion_graph(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb038d-5c3a-4bdb-9f6b-3831be0f6ac0",
   "metadata": {},
   "source": [
    "## Congrats!\n",
    "\n",
    "Congratulations! You've successfully created and tested a Reflexion pattern implementation using Strands multiagent orchestration. This system demonstrates:\n",
    "\n",
    "- **Iterative self-improvement** through systematic reflection and revision\n",
    "- **Multi-agent orchestration** with specialized roles and feedback loops\n",
    "- **Quality-driven processing** with configurable thresholds and iteration limits\n",
    "- **State management** across revision cycles for consistent improvement\n",
    "\n",
    "The reflexion pattern is particularly useful for applications requiring high-quality responses, such as content generation, problem-solving, and complex reasoning tasks where initial responses can be systematically improved through reflection and revision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678bb2b-56c5-43d5-b215-c466c79b87a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6eae98-019e-4a40-9888-30d8594edbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
